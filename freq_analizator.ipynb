{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from natasha import Segmenter, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger, MorphVocab, Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcc9081",
   "metadata": {},
   "source": [
    "# Вариант загрузки нескольких файлов из под-директории /temp относительно текущей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefbed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# конвеер загрузки нескольких xls файлов\n",
    "column = 'Текст'\n",
    "sheet_name = 'Base'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for final_xlsx in sorted(list(Path('./temp').rglob('*.xlsx'))):\n",
    "    df_temp = pd.read_excel(final_xlsx, sheet_name = sheet_name, index_col=0)[[column]].astype({column: 'string'})\n",
    "    df = pd.concat([df, df_temp])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3278e",
   "metadata": {},
   "source": [
    "# Вариант загрузки одного файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad259c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузка одного файла\n",
    "# название файла с текстовыми данными\n",
    "file_request = 'текстовые_данные_для_частотного_анализа.xlsx'\n",
    "# имя листа с данными в Excel-файле\n",
    "sheet_name = 'Лист1'\n",
    "# имя столбца с текстовыми данными\n",
    "column = 'Текст'\n",
    "\n",
    "df = pd.read_excel(file_request, sheet_name = sheet_name)[[column]].astype({column: 'string'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209caa19",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d41ba69",
   "metadata": {},
   "source": [
    "# Основной блок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843575dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('russian'))\n",
    "glagolitsa = 'АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'\n",
    "allowed = set(' ' + '_' + '-' + glagolitsa + glagolitsa.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b01c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_normalize(text: str) -> str:\n",
    "    \n",
    "    # Stage_1. Выделение именованных сущностей\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    for span in doc.spans:\n",
    "        span.normalize(morph_vocab)\n",
    "    normalize_name = ['_'.join(word.split()) if len(word.split()) > 1 else word for word in [_.normal for _ in doc.spans]]\n",
    "    unnormalize_name = [word.text for word in doc.spans]\n",
    "    assert len(normalize_name)==len(unnormalize_name), 'Assert Error'\n",
    "    \n",
    "    # Stage_2.  Удаление именованных сущностей из исходного корпуса\n",
    "    my_list=[]\n",
    "    for i, elem in enumerate(unnormalize_name):\n",
    "        my_list.append((elem,'_' + str(i)))\n",
    "        if elem in text:\n",
    "            text = text.replace (elem, '_' + str(i) + ' ')\n",
    "            \n",
    "    idx = [elem[1] for elem in my_list]  # эмпирические номера именованных объектов\n",
    "            \n",
    "    # Stage_3.  Лемматизация корпуса после удаления именованных сущностей\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    \n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "        \n",
    "    text = [_.lemma for _ in doc.tokens]\n",
    "    \n",
    "    # Stage_4. Сборка\n",
    "    final_list=[normalize_name[int(word[1:])] if word in idx else word for word in text]\n",
    "     \n",
    "    return ' '.join(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "\n",
    "    # Stage_1. Only russian text, ' ', '_' and '-'\n",
    "    text = ' '.join(''.join(l for l in text if l in allowed).split())\n",
    "\n",
    "    # Stage_2. Delete stopWords and short_words\n",
    "    text = ' '.join([word for word in text.split() if word not in stopWords and len(word) > 2])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lemma_corpus'] = df.apply(lambda x: clean_text(lemmatize_normalize(str(x[column]))), axis=1)\n",
    "corpus = df['Lemma_corpus'].str.cat(sep=' ')\n",
    "unique_key = set(corpus.split())\n",
    "unique_values = [0] * len(unique_key)\n",
    "unique_dict = dict(zip(unique_key, unique_values))\n",
    "print(len(corpus),' слов, из них ',len(unique_key),' уникальных. П = ', len(corpus)*len(unique_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05804ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "lentgh = len(unique_key)*df.shape[0]\n",
    "for word in unique_key:\n",
    "    for row in df['Lemma_corpus'].iteritems():\n",
    "        count+=1\n",
    "        if count % 1000000 == 0:\n",
    "            clear_output()\n",
    "            print(count,'done. Всего -',lentgh,'. ',round((count/lentgh)*100,2),' %')\n",
    "        if word in row[1].split():\n",
    "            unique_dict[word] += 1\n",
    "            \n",
    "data = pd.DataFrame([unique_dict]).T.sort_values(0, ascending=False)\n",
    "with pd.ExcelWriter('freq_analiz_sp.xlsx', options={'strings_to_urls': True}) as writer:  \n",
    "    data.to_excel(writer, sheet_name='Лист1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
